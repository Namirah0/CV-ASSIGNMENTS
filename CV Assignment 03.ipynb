{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66464a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?\n",
    "Ans: A stride 2 conv with the default padding (1) and ks (3) will reduce the activation map dimension by half. Formula:\n",
    "        (n + 2*pad - ks)//stride + 1. As the activation map dimension reduces by half we double the number of filters. This \n",
    "        results in no overall change in computation as the network gets deeper and deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv ?\n",
    "Ans: With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 \n",
    "    output numbers so there is not much learning since input and output size are almost the same. Neural networks will only \n",
    "    create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly \n",
    "    smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What data is saved by ActivationStats for each layer ?\n",
    "Ans: It records the mean, standard deviation, and histogram of activations of every trainable layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. How do we get a learner's callback after they've completed training ?\n",
    "Ans: They are available with the Learner object with the same name as the callback class, but in snake_case. For example, \n",
    "    the Recorder callback is available through learn.recorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are the drawbacks of activations above zero ?\n",
    "Ans: Activations near zero are problematic because it means we have computation in the model that’s doing nothing at all \n",
    "    (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to\n",
    "    the next layer… which will then create more zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Draw up the benefits and drawbacks of practicing in larger batches ?\n",
    "Ans: The gradients are more accurate since they’re calculated from more data, but a larger batch size means fewer batches per \n",
    "    epoch, which means less opportunities for the model to update weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9175d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why should we avoid starting training with a high learning rate ?\n",
    "Ans: Our initial weights are not well suited to the task we’re trying to solve. Therefore, it is dangerous to begin training \n",
    "    with a high learning rate: we may very well make the training diverge instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the pros of studying with a high rate of learning ?\n",
    "Ans: Training with a high learning rate gives two benefits:\n",
    "\n",
    "By training with higher learning rates, we train faster—a phenomenon Smith named super-convergence.\n",
    "By training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother \n",
    "(and therefore more generalizable) part of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Why do we want to end the training with a low learning rate ?\n",
    "Ans: A lower learning rate at the end of training allows us to find the best part of loss landscape and further minimize the\n",
    "    loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
