{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7491586",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How can each of these parameters be fine-tuned? • Number of hidden layers\n",
    "a.Network architecture (network depth)\n",
    "Ans: Neural Networks consist of layers where each layer has multiple neurons. The number of layers in a neural network defines its\n",
    "depth. Also, a neural network must have at least two layers: Input layer – it brings the input data into the system and \n",
    "represents the beginning of the neural network architecture.\n",
    "\n",
    "b.Each layer's number of neurons (layer width)\n",
    "The number of neurons in the input layer equals the number of input variables in the data being processed. The number of neurons in the output layer equals the number of outputs associated with each inputAns: \n",
    "\n",
    "c.Form of activation\n",
    "Activation Form means the form completed by Buyer and provided to Honeywell containing the Fee for the Hourly Airtime Package being purchased and the aircraft information necessary to activate the Service for Buyer.\n",
    "\n",
    "d.Optimization and learning\n",
    "In optimization, we care only about the data in hand. We know that finding the maximum value will be the best solution to our problem. In Deep Learning, we mostly care about generalization i.e the data we don't have.\n",
    "\n",
    "e.Learning rate and decay schedule\n",
    "The way in which the learning rate changes over time (training epochs) is referred to as the learning rate schedule or learning rate decay. Perhaps the simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value.\n",
    "\n",
    "f.Mini batch size\n",
    "The amount of data included in each sub-epoch weight change is known as the batch size. For example, with a training dataset of 1000 samples, a full batch size would be 1000, a mini-batch size would be 500 or 200 or 100, and an online batch size would be just 1.\n",
    "\n",
    "g.Algorithms for optimization\n",
    "A comprehensive introduction to optimization with a focus on practical algorithms for the design of engineering systems.\n",
    "This book offers a comprehensive introduction to optimization with a focus on practical algorithms. The book approaches optimization from an engineering perspective, where the objective is to design a system that optimizes a set of metrics subject to constraints\n",
    "\n",
    "h.The number of epochs (and early stopping criteria)\n",
    "This strategy of stopping early based on the validation set performance is called Early Stopping. This is explained with the below diagram. The validation set accuracy, however, saturates between 8 to 10 epochs. This is where the model can be stopped training.\n",
    "\n",
    "i.Overfitting that be avoided by using regularization techniques.\n",
    "Regularization is the answer to overfitting. It is a technique that improves model accuracy as well as prevents the loss of important data due to underfitting. When a model fails to grasp an underlying data trend, it is considered to be underfitting. The model does not fit enough points to produce accurate predictions.\n",
    "\n",
    "j.L2 normalization\n",
    "It may be defined as the normalization technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1. It is also called least squares.\n",
    "\n",
    "k.Drop out layers\n",
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "l.Data augmentation\n",
    "Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or using deep learning to generate new data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
