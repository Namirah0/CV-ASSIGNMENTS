{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5514cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE ?\n",
    "Ans: The InceptionNet architecture focusses on parallel processing and extraction of various feature maps concurrently. This is\n",
    "    the primarily attribute of the InceptionNet that differentiates it from all the other image classification models.\n",
    "    An inception network is a deep neural network with an architectural design that consists of repeating components referred \n",
    "    to as Inception modules. As mentioned earlier, this article focuses on the technical details of the inception module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Describe the Inception block ?\n",
    "Ans: An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put \n",
    "    simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a \n",
    "    single image block, which we then concatenate and pass onto the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL) ?\n",
    "Ans: The 1x1 convolution can be used to address this issue by offering filter-wise pooling, acting as a projection layer that \n",
    "    pools (or projects) information across channels and enables dimensionality reduction by reducing the number of filters \n",
    "    whilst retaining important, feature-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd96c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE ?\n",
    "Ans: he number of input variables or features for a dataset is referred to as its dimensionality.\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.\n",
    "\n",
    "More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse \n",
    "of dimensionality.\n",
    "\n",
    "High-dimensionality statistics and dimensionality reduction techniques are often used for data visualization. Nevertheless \n",
    "these techniques can be used in applied machine learning to simplify a classification or regression dataset in order to better \n",
    "fit a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Mention three components. Style GoogLeNet ?\n",
    "Ans: Google Net (or Inception V1) was proposed by research at Google (with the collaboration of various universities) in 2014 \n",
    "    in the research paper titled “Going Deeper with Convolutions”. This architecture was the winner at the ILSVRC 2014 image \n",
    "    classification challenge. It has provided a significant decrease in error rate as compared to previous winners AlexNet \n",
    "    (Winner of ILSVRC 2012) and ZF-Net (Winner of ILSVRC 2013) and significantly less error rate than VGG (2014 runner up). \n",
    "    This architecture uses techniques such as 1×1 convolutions in the middle of the architecture and global average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d95401",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Using our own terms and diagrams, explain RESNET ARCHITECTURE ?\n",
    "Ans: Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner\n",
    "    of ImageNet challenge in 2015. The fundamental breakthrough with ResNet was it allowed us to train extremely deep neural \n",
    "    networks with 150+layers successfully. Prior to ResNet training very deep neural networks was difficult due to the problem\n",
    "    of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What do Skip Connections entail ?\n",
    "Ans: Skip Connections (or Shortcut Connections) as the name suggests skips some of the layers in the neural network and feeds\n",
    "    the output of one layer as the input to the next layers. Skip Connections were introduced to solve different problems in \n",
    "    different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is the definition of a residual Block ?\n",
    "Ans: Similar to LSTM Block. ( img src) Residual blocks are basically a special case of highway networks without any gates in \n",
    "    their skip connections. Essentially, residual blocks allow memory (or information) to flow from initial to last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. How can transfer learning help with problems ?\n",
    "Ans:Transfer learning models focus on storing knowledge gained while solving one problem and applying it to a different but \n",
    "    related problem. Instead of training a neural network from scratch, many pre-trained models can serve as the starting point\n",
    "    for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac04a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is transfer learning, and how does it work ?\n",
    "Ans: transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a\n",
    "    new task. To put it simply—a model trained on one task is repurposed on a second, related task as an optimization that \n",
    "    allows rapid progress when modeling the second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.HOW DO NEURAL NETWORKS LEARN FEATURES? 11. HOW DO NEURAL NETWORKS LEARN FEATURES ?\n",
    "Ans: Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. \n",
    "    In simple terms, you can think of the data flowing from the input node to the output node through many different paths in \n",
    "    the neural network.\n",
    "    Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using \n",
    "    algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and – over time – \n",
    "    continuously learn and improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING ?\n",
    "Ans:Fine-tuning a pre-trained model can be more effective than training a model from scratch in certain situations. For \n",
    "    example, if you have a small dataset and want to use a model that has already been trained on a similar task, fine-tuning \n",
    "    can help you achieve better performance than training from scratch. However, if your dataset is very different from the one\n",
    "    used to pre-train the model, or if the task you want to perform is very different from the pre-training task, training from\n",
    "    scratch might be the better option. Ultimately, the choice between fine-tuning and training from scratch will depend on the\n",
    "    specific problem and dataset you are working with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
